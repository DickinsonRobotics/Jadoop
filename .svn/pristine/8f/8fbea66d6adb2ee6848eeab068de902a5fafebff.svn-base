<html>
<head>
<title>Jadoop</title>
</head>
<body>
<center>
<h1>Jadoop</h1>
</center>

<h2>Jadoop Overview</h2>

<p>Jadoop is a Java API to facilitate the use of a <a href="http://hadoop.apache.org/">hadoop</a> cluster
for grid computing applications. Jadoop makes it easy to create a Java application that: specifies a list 
of command line instructions, executes them in parallel on a hadoop cluster, and collects the output
generated by the execution of the instructions.

<p>Of course hadoop is much more than this and there are many special purpose grid computing solutions
available. Thus, Jadoop may not be the right solution for your application.  But if you have access to 
an existing hadoop cluster and need to run a grid computing application from Java, then Jadoop may be 
a solution to consider.

<h2>Jadoop Use</h2>

<p>Jadoop processes <i>jobs</i> which are a collection of related <i>tasks</i>.  When a job is executed
on the cluster, the tasks in the job are distributed across the cluster nodes and executed in parallel. 

<p>Jadoop uses two Java classes to represent tasks and jobs:
<UL>
<p><LI><b>HadoopGridTask</b>: A HadoopGridTask represents a single command line that will be executed,
via a HadoopGridJob, on one of the nodes in the cluster. The result of a HadoopGridTask can include the 
standard output and/or the standard error generated by the execution of the command line.  A timeout
period must be specified for each HadoopGridTask. When a HadoopGridTask times out, the task is terminated
and any output generated up to that point is returned.

<p><LI><b>HadoopGridJob</b>: A HadoopGridJob collects together one or more, typically but not 
necessarily, related HadoopGridTasks to be executed on the cluster.  The HadoopGridJob also provides 
mechanisms for: packaging data files, scripts and code libraries needed by the tasks; initiating the
execution of the tasks; and monitoring the status of the tasks.  A timeout period can be specified
for the HadoopGridJob.  If a HadoopGridJob times out then all of the tasks are terminated and no 
results are returned for any tasks. Thus, it is usually preferable to not set a timeout for the job
but to rely on timeouts for the individual HadoopGridTasks to ensure that a job does not run forever.
To reflect this the default timeout for HadoopGridJob's is set very large, so as to have no practical 
effect and an explicit call to setJobTimeout is necessary to change the timeout.
</UL>

<p>A Jadoop application will typically include the following steps:
<OL>
<LI>Create a HadoopGridJob object. Use this object to configure the job, including specifying any files (input data, 
shell scripts or code libraries) or archives (jar, zip) that will be needed by the tasks in the job.

<LI>Create a HadoopGridTask object for each command to be executed on the cluster and add it to the HadoopGridJob.

<LI>Use the HadoopGridJob to run job on the hadoop cluster.

<LI>Monitor the HadoopGridJob, waiting for all of the tasks to finish (successfully complete, time out or fail).

<LI>Use the HadoopGridTask objects to collect and process the results from each command.
</OL>

<p>Some basic examples of working Jadoop applications can be found in the jadoop.examples package.

<h2>Sample Jadoop Applications</h2>

<p>The examples directory contains a number of simple illustrative examples of using
Jadoop. These include:
<UL>
<LI>Hostnames: Creates a number of tasks that each run the unix 
hostname command. The output of each task is the name of the host on which it was run.  The
job then displays the results of each of the tasks.
<LI>Easter: Creates a a number of tasks that run the unix ncal -e (compute the date of
Easter in a given year). The year for each task is unique and is passed to ncal on the command
line of the task.  The job then displays the date of Easter in each of the years.
<LI>CoinFlipExperiment: Creates a number of tasks that each run a java program (CoinFlipTask). 
The CoinFlipTask accepts a number of coin flips to perform and prints the number of heads that
are obtained. The job then collects the results of each task, displays them and computes the 
average number of heads obtained across trials.  This example illustrates the primary use case for
Jadoop, running multiple trials of an experiment in parallel on the hadoop cluster.
</UL>

<h2>Jadoop Internals</h2>

<p>Jadoop works by interacting with the hadoop HDFS and (mildly abusing?) the map/reduce framework. 
The following list connects the steps in a typical Jadoop application, as outlined above 
(the numbers above correspond to those below), with the hadoop mechanisms that are used to 
implement the functionality:
<OL type>
<LI>As the HadoopGridJob is configured it maintains a list of any files or archives that will 
be required for the execution of the tasks are noted.  Nothing is done with these files/archives
at this point. The JadoopGridJob simply maintains a list of them for later processing.

<LI>HadoopGridTasks are added to the job with an associated key. The HadoopGridJob maintains a map
of the tasks accessible by key, but it does nothing further with the tasks at this point.

<LI>When the runJob method of the HadoopGridJob is invoked it constructs and configures
a hadoop map/reduce Job (org.apache.hadoop.mapreduce.Job) object that will be used to 
execute/monitor/control the job on the hadoop cluster. The job is then submitted to the
hadoop cluster using the Job object. More specifically the runJob method carries out the 
following steps:
<OL type=a>
<LI>A temporary working directory is created on the hadoop HDFS to hold the task inputs, outputs
and necessary files.  
<LI>The org.apache.hadoop.mapreduce.Job object being used is configured as follows:
<UL>
<LI>The input directory set to a directory named "input" in the temporary working directory.
<LI>The output directory set to a directory named "output" in the temporary working directory.
<LI>The Mapper class set to be of type HadoopGridTaskRunner. This subclass of
org.apache.hadoop.mapreduce.Mapper takes in the key 
as a Text object and the command as a TextArrayWritable object. The TextArrayWritable class 
is a sub-class of the org.apache.hadoop.io.ArrayWritable type. The first three entries in the
TextArrayWriteable are parameters to the HadoopGridTaskRunner that indicate whether standard input 
and/or standard output is to be captured and a timeout (in ms) for the task. The remaining entries
in the TextArrayWritable
object are strings making up the command to be executed.  The full command is constructed by appending these one 
after the other, delimited by spaces.  The output of this Mapper is the key, also as a Text object
and a MapWritable containing:
<Table border=1>
<tr><th>Key</th><th>Type</th><th>Value</th></tr>
<tr><td>EV</td><td>ByteWritable</td><td>The exit value from the command execution, or -1 if the task timed out.</td></tr>
<tr><td>TO</td><td>BooleanWritable</td><td>True if the task execution has timed out.</td></tr>
<tr><td>SO</td><td>Text</td><td>The standard output from the command execution (if captured)</td></tr>
<tr><td>SE</td><td>Text</td><td>The standard error from the command execution (if captured)</td></tr>
</table>
<LI>No reduce tasks. This is where the map/reduce model is mildly abused. The type of grid applications
for which Jadoop was designed typically just need to run and generate output.  The controlling
application then does whatever processing is necessary to merge the results. Very often this
is the computation of various summary statistics, and is handled perfectly well by the application
rather than in a reduce step on the cluster (though some standard, easily assigned, reducers might make a nice
feature addition - e.g. column averages / standard deviations).
<LI>The input format class set to be of type SingleRecordSplitSequenceFileInputFormat. This 
sub-class of org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat ensures that each 
key, TextArrayWritable pair written to the tasks.seq file (see below) is treated as its own task 
(i.e. is scheduled independently by hadoop to one of the nodes).  Thus, all commands can 
potentially execute in parallel.
<LI>The output key class is set to be Text, and the task output will use the same key provided
to the mapper.
</UL>

<LI>All of the files and archives needed for the HadoopGridJob are copied to the temporary working
directory on the hadoop HDFS. These are then made available, 
via hadoop's distributed cache, in the working directory of each map task when it is run. Thus, 
each task has direct access to these files at runtime. In fact, the task can be, and often is, 
to execute one of these files.

<LI>The HadoopGridJob retrieves the key, flags and command from each of the HadoopGridTasks in the job.  
The HadoopGridJob writes each of these key, flags and commands into the tasks.seq file in the 
input directory for the Job on the hadoop HDFS. Because the input format is of type 
SingleRecordSplitSequenceFileInputFormat each key, TextArrayWritable pair becomes one map task to be
scheduled.

<LI>The job is submitted to the cluster via the submit method in the Job class. For each task
scheduled a HadoopGridTaskRunner object (on a node in the cluster) 
will run the the command, wait for it to finish (or timeout) and then capture standard output
and/or standard error as indicated by the flags.  If the task times out the exit value will be
-1 and the TO element of the MapWritable will be true, any output generated before the timeout
will be returned (if captured).

<LI>A HadoopGridJob.JobMonitor thread is started.  This thread polls the hadoop Job periodically
to determine if it has finished, and also checks if the HadoopGridJob has timed-out or been 
terminated.  If the HadoopGridJob is set to wait for completion, the JobMonitor thread is joined.
If not, the runJob method returns and the progress of the job can be monitored from the calling
code using methods in the HadoopGridJob method (see below).

<LI>When the job completes (but not if it times out or is terminated) the JobMonitor thread
retrieves, from the hadoop HDFS, any results that are available.  The results for each task
are placed into the corresponding HadoopGridTask object.  If the job is terminated or timed
out no results are available for any tasks.

<LI>Finally, the JobMonitor thread cleans up the HDFS by deleting the temporary working
directory and all of its contents.
</OL>

<LI>A number of methods in the HadoopGridJob class can be used to monitor the progress
of the job (e.g. isRunning, isComplete, isSuccessful).  The getStatus method returns 
a org.apache.hadoop.mapreduce.JobStatusJobStatus object that provides more detailed information.  
A running display of the progress can also be displayed in the console by invoking the 
monitorAndPrintJob method.

<LI>Finally, the HadoopGridJob can be used to access the HadoopGridTask objects that 
now contain the results.  The application then processes these results in whatever way 
is appropriate. If there are failed tasks, a new HadoopGridJob can easily be created to retry
them.
</OL>
</body>
</html>